<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Reconstruct a 3D scene with SDF 3D Transformers from monocular RGB images."/>
    <title>3D-Former: Monocular Scene Reconstruction with 3D SDF Transformers</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: #fdfcf9 no-repeat fixed top left;
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">3D-Former: Monocular Scene Reconstruction with SDF 3D Transformers</h2>
            <h4 style="color:#6e6e6e;"> ICLR 2023</h4>
            <!-- <h5 style="color:#6e6e6e;"> (Oral Presentation and Best Paper Candidate)</h5> -->
            <hr>
            <h6> <a href="https://weihao-yuan.com/" target="_blank">Weihao Yuan</a>, 
                 <a target="_blank">Xiaodong Gu</a>, 
                <a target="_blank">Heng Li</a>,
                <a target="_blank">Zilong Dong</a>,
                <a href="https://sites.google.com/site/zhusiyucs/home" target="_blank">Siyu Zhu</a>,
            <p> Alibaba Group &nbsp;&nbsp; 
                <!-- <sup>2</sup>Simon Fraser University -->
                <br>
            </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2301.13510" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code" href="https://github.com/alibaba-damo-academy/former3d" role="button" 
                    target="_blank" disabled=1>
                <i class="fa fa-github-alt"></i> Code </a> </p>
              </div>
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Supplementary</a> </p>
              </div> -->
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
            <!-- <h6 style="color:#8899a5"> Reconstruct a 3D scene with SDF 3D Transformers from monocular RGB images.</h6> -->
            <!-- <video poster="images/header-vid-poster.png" width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" id="header_vid"> -->
                   
            <div><b style="color:#fd5638; font-size:large" id="demo-warning"></b>
            <br>
            </div>
              <!-- <br><br> -->
          <p class="text-justify">
            Monocular scene reconstruction from posed images is challenging due to the complexity of a large environment.
            Recent volumetric methods learn to directly predict the TSDF volume and have demonstrated promising results in this task.
            However, most methods focus on how to extract and fuse the 2D features to a 3D feature volume, but none of them improve the way how the 3D volume is aggregated.
            In this work, we propose an SDF transformer network, which replaces the role of 3D CNN for better 3D feature aggregation.
            To reduce the explosive computation complexity of the 3D multi-head attention, we propose a sparse window attention module, where the attention is only calculated between the non-empty voxels within a local window.
            Then a top-down-bottom-up 3D attention network is built for 3D feature aggregation, where a 
            dilate-attention structure is proposed to prevent geometry degeneration, and two global modules are employed to equip with global receptive fields.
            The experiments on multiple datasets show that this 3D transformer network generates a more accurate and complete reconstruction, which outperforms previous methods by a large margin.
            Remarkably, the mesh accuracy is improved by 41.8%, and the mesh completeness is improved by 25.3% on the ScanNet dataset.
          </p>
          
        </div>
      </div>
    </div>
  </section>
  <br>


  <!-- method -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Overview Pipeline</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" src="images/framework.png" alt="Framework" width=800>   
            <hr style="margin-top:0px">
            <p class="text-justify">
              The overview of the 3D reconstruction framework. The input images are extracted to features by
              a 2D backbone network, then the 2D features are back-projected and fused to 3D feature volumes, which are
              aggregated by our 3D transformer and generate the reconstruction in a coarse-to-fine manner.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Network Structure</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" src="images/downup.png" alt="Network" width=300>   
            <hr style="margin-top:0px">
            <p class="text-justify">
              The structure of the 3D transformer. “S-W-Attn” denotes sparse window attention.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Sparse Window Attention</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" src="images/attention.png" alt="Attention" width=800>   
            <hr style="margin-top:0px">
            <p class="text-justify">
              (a) Illustration of the sparse window attention. For calculating the attention of the current voxel (in
              orange), we first sparsify the volume using the occupancy prediction from the coarser level, and then search the
              occupied voxels (in dark blue) within a small window. The attention is hence computed based on only these
              neighbor occupied voxels. (b) Illustration of the dilate-attention in a 2D slice. We dilate the occupied voxels
              and calculate the attention of these dilated voxels (in yellow) to maintain the geometry structure.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>


  <!-- results -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Results</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" src="images/results1.png" alt="Results" width=800>   
            <img class="img-fluid" src="images/metrics.png" alt="Results" width=500>  
            <hr style="margin-top:0px">
        </div>
      </div>
    </div>
  </section>
  <br>


  <!-- conclusion -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Discussions</h3>
            <hr style="margin-top:0px">
            <p class="text-justify">
              Due to the volume representation, our framework is limited by the trade-off between the resolution
              of the volume and the memory consumption.
              The voxel size is set to 4 cm, such that the geometry details less than 4 cm are hard to be recovered.
              A smaller voxel size would cost more memory, and obtain better reconstruction with more details.
            </p>
            <p class="text-justify">
              In addition, this 3D transformer structure could be used to aggregate any 3D feature volume, thus it could
              be applied to more 3D tasks in the future, such as 3D segmentation.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>


  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@inproceedings{yuan2022former3d,
  title={3D-Former: Monocular Scene Reconstruction with 3D SDF Transformers},
  author={Yuan, Weihao and Gu, Xiaodong and Li, Heng and Dong, Zilong and Zhu, Siyu},
  booktitle={Proceedings of the International Conference on Learning Representations},
  pages={},
  year={2023}
}
</code></pre>
          <hr>
      </div>
    </div>
  </div>

  <!-- ack -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgements</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            We would like to specially thank 
          </p>
      </div>
    </div>
  </div> -->

  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>

  <script type="text/javascript">
    function changePlaybackSpeed(speed)
        {
            document.getElementById('inspect_vid').playbackRate = speed;
        }
        // changePlaybackSpeed(0.25)

    var demo = document.getElementById("header_vid");
    var startTime;
    var timeout = undefined;
    demo.addEventListener("loadstart", function() {
      startTime = Date.now();
      timeout = setTimeout(function () {
        var demoWarning = document.getElementById("demo-warning");
        var giteeLink = document.createElement("a");
        giteeLink.innerText = "mirror hosted in mainland China";
        giteeLink.href = "https://project-pages-1255496016.cos-website.ap-shanghai.myqcloud.com/neuralrecon/";
        // var bilibiliLink = document.createElement("a");
        // var youtubeLink = document.createElement("a");
        // bilibiliLink.innerText = "BiliBili";
        // bilibiliLink.href = "";
        // youtubeLink.innerText = "YouTube";
        // youtubeLink.href = "";

        demoWarning.append("Loading the videos took too long, you can optionally visit this site in the ", giteeLink, ".");
        // demoWarning.append("Loading the video took too long, you can optionally watch it on Bilibili", bilibiliLink, " or YouTube", youtubeLink, ".");
        clearTimeout(timeout);
        timeout = undefined;
      }, 6000);
    });
    demo.addEventListener("loadeddata", function() {
      if (timeout) {
        clearTimeout(timeout);
        timeout = undefined;
      }
    });
//     var source = document.createElement("source");
//     source.setAttribute("src", "/videos/web-scene2.m4v");
//     source.setAttribute("type", "video/webm");
//     demo.appendChild(source);
  </script>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
